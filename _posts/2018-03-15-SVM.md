---
layout: post
title: "实现出真知——理解SVM"
date: 2018-03-15   
tag: 机器学习基础
---

### 一、前言
svm一直是我很困惑的一个算法，一方面各种资料上都说svm是一个以强大的算法，但是在我目前为止的科研过程中并没有关于它的应用场景，所以它就成了一个悬案。时间久了，个中滋味，哎，只能说很难受。有尝试过去推公式，也把推导过程写在小本本上了。但是后来再翻的时候，一头雾水。写得是啥，一脸黑人问号（此处就不放了，有点丢人[/捂脸]）。就重新看了些公式推导过程，又是一头雾水，什么拉格朗日乘子法、KKT条件、SMO算法等等，这些都干嘛的，为啥要这么干？越看越晕，也不是说看不懂，就是不知道该怎么看。
既然不知道怎么去看，索性就直接实现。直接看代码，省得看那一坨坨符号公式晕。但是代码好像也挺长的也有一堆名词看不懂呢，那就直接伪代码、算法。
于是，新世界的大门就这么打开了...


### 二、算法／伪代码
直接上算法和计算过程
线性可分支持向量机学习算法

> 输入： 线性可分训练集 $T=(x_1,y_1),(x_2,y_2),...,(x_N,y_N)$, 其中，$x_i\in X=R^{n}, y_i\in Y={+1,-1},i=1,2,...,N$;
> 
> 输出：分离超平面和分类决策函数
> - 构造并求解约束最优化问题：
> 
> $$\underset{\alpha }{min}  \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_iy_j(x_i\cdot x_j)- \sum_{i=1}^{N}\alpha_{i}$$
> 
> $$s.t. \quad \sum_{i=1}^{N}\alpha_{i}y_i=0,$$
> 
> $$\alpha_i\geq 0,i=1,2,...,N$$
> - 求解最优解：
> $$\alpha^{*}=(\alpha_{1}^{*},\alpha_{2}^{*},...,\alpha_{N}^{*})$$
> - 计算  $$\omega^{*},b^{*}$$
> 
> $$\omega^{*}=\sum_{i=1}^{N}\alpha_{i}^{*}y_ix_i;$$
> - 选择 
> 
> $$\alpha^{*}$的一个正分量 $\alpha_{j}^{*}$$，计算
> 
> $$b^{*}=y_j-\sum_{i=1}^{N}\alpha_{i}^{*}y_i(x_i\cdot x_j)$$
> - 求出分离超平面和分类决策函数,
>     - 分离平面：
> $$\omega^{*}\cdot x_i + b^{*}=0 $$
> 
>     - 决策函数：$$f(x)=sign(\omega^*\cdot x+b^*)$$


定义(支持向量)： 从上面可以看出，$$\omega^{*}$ 和 $b^{*}$$ 只依赖于训练数据中 $$\alpha_{i}^{*} > 0$$ 的点，
我们将训练数据中对应 $$\alpha_{i}^{*} > 0$$ (互补松弛条件可知，此时有 $$\omega^{*}\cdot x_i + b^{*}=1$$ ) 的实例点 $$ (\alpha_i^*\in R^n )$$ 称为支持向量。


**然后我们再通过一个例子来说明这个计算过程：** 
已知数据集正例点为 $x_1=(3,3)^T, x_2=(3,4)^T ,$ 负例点为 $x_3=(1,1)^T$ ，试求最大间隔分离超平面。
解： 
1. 构造数据集约束最优化问题：
$\underset{\alpha }{min}  \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_iy_j(x_i\cdot x_j)- \sum_{i=1}^{N}\alpha_{i}=\frac{1}{2}(18\alpha_{1}^{2}+25\alpha_{2}^{2}+2\alpha_{3}^{2}+42\alpha_{1}\alpha_{2}-12\alpha_1\alpha_3-14\alpha_2\alpha_3)-\alpha_1-\alpha_2-\alpha_3$
$s.t. \alpha_1+\alpha_2-\alpha_3=0,$
$\alpha_i\geq 0,i=1,2,3$
2. 解最优化问题
将 $\alpha_3=\alpha_1+\alpha_2$ 带入目标函数并记为,
$$s(\alpha_1,\alpha_2)=4\alpha_1^2+\frac{13}{2}\alpha_2^2+10\alpha_1\alpha_2-2\alpha_1-2\alpha_2$$
对 $\alpha_1,\alpha_2$ 分别求偏导并令其为 0，可以知道 $s(\alpha_1,\alpha_2)$  在点 $(\frac{3}{2},-1)^T$ 取极值，但是该点的 $\alpha_2$ 不满足约束 $\alpha_i\geq 0 $ ，所以最小值应在边界上达到：当 $\alpha_1=0$  时，最小值 $s(0,\frac{2}{13})=-\frac{2}{13}$；当 $\alpha_2=0$ 时，相应的 $s(\frac{1}{4},0)=-\frac{1}{4}$  ，于是，在  $\alpha_1=\frac{1}{4},\alpha_2=0$时，$s$ 达到最小值，此时 $\alpha_3=\alpha_1+\alpha_2=\frac{1}{4}$ 。对应的实例点 $x_1,x_3$  为支持向量，此时,
$$\omega_1^{*}=\omega_2^{*}=\frac{1}{2}$$
$$b^*=-2$$
分离超平面为：
$$\frac{1}{2}x^{(1)}+\frac{1}{2}x^{(2)}-2=0$$
分离决策函数为：
$$f(x)=sign(\frac{1}{2}x^{(1)}+\frac{1}{2}x^{(2)}-2)$$


### 三、一些解释
通过刚刚的算法，我们其实已经能够书写svm了，但是直接看算法心里还是会有些不痛快，例如最优化方程是如何得来的，不是说好的，支持向量机找出所有点距“分割面”的最小距离和最大的这样一个“分割面”吗？最优化方程怎么看不出一点距离公式的影子？
###### 1. 原始的距离公式的推导 

数据点到超平面的间隔：
$$d = \frac{1}{\left \| w \right \|} \left | w^Tx+b \right |$$
为了表是任意数据点到“分割面”的距离，通过将数据标签设为 $+1/-1$ 来将距离统一用下面的公式表示：
$$d = y_i \cdot (w^Tx+b) \cdot \frac{1}{\left \| w \right \|}$$
这样无论数据点是否在“分割面”的上方还是下方，$d$的值都是大于0的，而且数据点距离“分割面”越大$d$就越大。
间隔公式有了，现在的目标就是找到一组最好的$w$ 和 $b$ 确定“分割面”，使**支持向量**距离此平面的的**间隔最大**。
直接使用公式表示：
$$ arg\ \underset{w,b}{max} \{\underset{n}{min}(y_i \cdot(w^T+b)) \cdot \frac{1}{\left \| w \right \|} \} $$
通俗解释就是，在所有数据点中找出距“分割面”最近的点，使它们的和最大，就是这个公式干的事情。需要求解的参数使$w$ 和 $b$ 。
还是离最终的目标函数差的很远，不过不要着急。

先说说几何间隔和函数间隔，几何间隔：$\hat{\gamma_i}=y_i(\omega x_i^T+b)$ ; 函数间隔：$\gamma_i=y_i(\frac{\omega x_i^T}{\left \| w \right \|}+\frac{b}{\left \| w \right \|})$
我们的优化目标是最大化数据点到超平面的间隔，这里，可以把**最小化的部分（寻找支持向量）放到约束条件中**，有，


$arg\ \underset{w,b}{max} \frac{\hat{\gamma}}{\left \| w \right \|}$
$subject \ to,$
$y_i \cdot(\omega x_i^T+b) \geq \hat{\gamma}, \ i=1,2,...,k$
其中 $\hat{\gamma}$ 是支持向量到超平面的函数间隔。
因为超平面有一个性质，等比例的扩大或者缩小并不会改变超平面的位置，例如$2x+2y+z+5=0$ 和 $\frac{3}{2}+y+\frac{1}{2}z+\frac{5}{2}=0$ 是相同的平面。所以，
我们将 $w$ 和 $b$ 都除以 $\hat{\gamma}$ ，于是有，
$ arg\ \underset{w,b}{max} \frac{1}{\left \| w \right \|}$
$ subject \ to, $
$ y_i \cdot(\omega x_i^T+b) \geq 1, \ i=1,2,...,k $
然后为了计算和求导方便，将最大化问题改为求最小值：
$ arg\ \underset{w,b}{min} \frac{1}{2}\left \| w \right \|$
$ subject \ to, $
$ -y_i \cdot(\omega x_i^T+b) \geq 1, \ i=1,2,...,k $
是不是越来越像，下一步也是最关键和最难理解的地方，也就是将上述线性不等式的约束下的**二次优化**问题通过**拉格朗日乘子法**来花去我们优化目标的**对偶形式**。3个读起来很晦涩的点，其实不要紧，不妨碍我们理解svm。
###### 2. 对偶形式的推导

拉格朗日乘子法是一种寻找多元函数在其变量受到一个或者多个条件的约束的极值的方法。而KKT条件是拉格朗日乘子法在**约束条件为不等式**的一种延伸。
举例说明一下吧，
带等式约束的优化问题，
优化函数为：$\underset{x}{min}\ f(x), subject\ to\ h(x)=0$ ,拉格朗日乘子法就是通过引入新的位置变量将上式的约束条件一起放到目标函数中：$ L(x, \lambda)=f(x)+\lambda h(x) subject\ to \ h(x)=0$ ，然后通过求解方程组：$\nabla L(x, \lambda)=0; h(x)=0$ 便可得到局部最小值的必要条件。
带不等式约束的优化问题（KKT 条件），优化函数为：$ \underset{x}{min}f(x) \ subject\ to \  g_1(x) \leq 0, g_2(x) \leq 0 $     ,对应的拉格朗日函数：$ L(x, \mu_1,\mu_2) = f(x)+\mu_1g_1(x)+\mu_2g_2(x) $
KKT条件（具有局部最优点的必要条件）为：
1. $\mu_1 \geq 0, \mu_2 \geq 0$
2. $\nabla f(x)+\mu_1 \nabla g_1(x) + \mu_2 \nabla g_2(x)=0$
3. $ \mu_1 g_1(x) + \mu_2 g_2(x)=0$

然后我们继续svm。
现在要求解svm：
$ arg\ \underset{w,b}{min} \frac{1}{2}\left \| w \right \| \tag{1}$
$ subject\ to, $
$ 1+y_i \cdot (wx_i^T+b) \leq 0 , i=1,2,...,k \tag{2}$
对应的拉格朗日函数：
$$ L(\omega,b,\alpha) = \frac{1}{2} {\left \| w \right \|}^2 - \sum^{N}_{i=1}\alpha_i[y_i(\omega^T x_i+b)-1] \tag{3}$$ 
其中，$\alpha_i \geq 0$
然后我们要获取目标函数的对偶问题，通过求解对偶问题的解来获取SVM的最优解。即，
$$ \underset{\alpha}{max}\ \underset{w, b}{min}\ L(w, b, \alpha) $$
这个公式叫做拉格朗日对偶问题（Lagrangian duality），解释起来有点绕，我的理解是，拉格朗日函数中不同的 $\alpha$ 有很多个对应了不同个关于 $w, b$ 的函数，$min$ 后面那一堆表示在在没有引入拉格朗日算子之前优化关于函数 $(1)$ ，求取最小值，但是怎么保证这个$min$ 里面的函数值最小呢，也就是找出对应不同的 $\alpha$ 的最大值，想想看最大值都比一个数小，那其他的数是不是更比这个 数小呢？那么这个最大值是不是就是最优解呢？这就是 $max$ 后面那一堆的意思。
这样，我们先不管 $\alpha$ ,  求 $(3)$ 关于 $w, b$的导数。
1. 对于 $w$:  
$$\nabla_wL(w,b,\alpha)=w-\sum_{i=1}^{N}\alpha_iy_ix_i=0, 得到\  w=\sum_{i=1}^{N}\alpha_iy_ix_i \tag{4}$$
2. 对于 $b$:  
$$ \frac{\partial L}{\partial b} =-\sum_{i=1}^{N}\alpha_iy_i =0 \tag{5}$$

将 $(4),(5)$ 代入 $(3)$, 得，
$$ w^Tw=(\sum_{i=1}^N\alpha_iy_ix_i)^T(\sum_{j=1}^{N}\alpha_iy_ix_i)=\sum_{i=1}^{N}\sum_{j=1}^{N}y_iy_j\alpha_i\alpha_j \left \langle x_i,x_j \right \rangle \tag{6}$$
$$ W(\alpha) = \sum_{i=1}^{N}\alpha_i-\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}y_iy_j\alpha_i\alpha_j \left \langle x_i,x_j \right \rangle \tag{7}$$
对偶问题：
$$ \underset{\alpha}{max}\ W(\alpha) $$
$$ subject\ to, \ \alpha_i \geq 0 ; \sum_{i=1}^{N} y_i\alpha_i=0$$
OK, 最后得到我们得优化函数：
$$ arg\ \underset{\alpha}{max}[\sum_{i=1}^{N}\alpha_i-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}y_iy_j\alpha_i\alpha_j \left \langle x_i,x_j \right \rangle] $$
$$ subject\ to, \ \alpha_i \geq 0 ; \sum_{i=1}^{N} y_i\alpha_i=0$$
### 四、 一些补充
还有一些知识需要说明的，例如松弛因子的引入，这个松弛因子的作用是，消除数据集中的异常点，不能因为个别的异常点去改变整个世界！
还有快速求解 $\alpha$ 的SMO算法。原理待看。

### 五、参考
1. [向量机](https://www.zybuluo.com/curiousbull/note/339883)
2. [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)
3. [支持向量机SVM通俗理解（python代码实现）](https://blog.csdn.net/csqazwsxedc/article/details/71513197)
4. [如何通俗地讲解对偶问题？尤其是拉格朗日对偶lagrangian duality？](https://www.zhihu.com/question/58584814)