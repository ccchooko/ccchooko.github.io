---
layout: post
title: "神经网络推导及python实现"
date: 2018-03-02   
tag: 机器学习基础
---

## 一、神经网络的结构

![](https://ws3.sinaimg.cn/large/0069RVTdly1fupatt9gwmj30q80irn05.jpg)

通常由3部分构成，输入层、输出层、隐藏层。在隐藏层节点，有激活函数作为转换（如果没有这个转换就与线性回归等价了）。
符号说明：

1. 输入层$x_1,x_2,...,x_n$，隐藏层神经元包括两个部分，该神经元接收的值($z_j^{(l)}$)和该神经元输出的值($a_j^{(l)}$)，神经元和神经元相连的权重$w_{ij}$, 还有每一个隐藏层的偏置项（$b_j^{(l)}$）。

2. 基本公式：

    * 按元素表示：

    > 第l层第j个神经元的值为第l-1层k个神经元的值a_k^{(l-1)}和分别与权重w_{jk}^l的乘积之和再加上第l层第j个神经元的偏置项b_j。
    

	$$ z_j^l = \sum_{i=1,2,...,n}^{d(L)}w_{ij} \cdot a_{ij}^{(l-1)}+b_j^l $$

	$$ a_j^l=f(z_j^{(l)})=\frac{1}{1+e^{-z_j^{(l)}}} $$

	

    * 按矩阵表示：

$$a^l=\sigma(w^la^{l-1}+b^l)$$

## 二、四个重要的神经网络计算公式

> 对于这四个公式是我们通过后向传播求解权重w的梯度和偏置项b的梯度的关键，进而才能求出每一个w和b的值。公式(1)，它的推导来自于平方损失函数C。其实这些公式都很简单，都是基于链式推导，都是基于我们求损失函数关于w的偏导的初衷来的。也就是说，公式(1)是用来求最后一层的$\delta^L$,然后一步一步的往前推。公式(2)用来迭代求前面的$\delta$, 公式(3)(4)用来求w，b的梯度，从而完成一次后向传播

$$\delta^L= \frac{\partial C}{\partial z^L}= \frac{\partial C}{\partial a^L} \cdot  \frac{\partial a^L}{\partial z^L}=\triangledown_aC \odot \sigma^{'}(z^L)  \tag{1}, 其中，\odot是Hadamard Product$$

$$\delta^l=((w^{l+1})\cdot \delta^{l+1})\odot \sigma^{'}(z^L) \tag{2}$$

$$\frac{\partial C}{\partial b_j^l}=\delta_j^l \tag{3}$$

$$\frac{\partial C}{\partial w_{jk}^l}=a_k^{l-1} \cdot \delta_j^l \tag{4}$$

- 推导一下公式（1）（2）的由来吧。

    - 输出$a^{(L)}$, 神经网络的输出值。真实值为y

    - 损失函数：$E=1/2(y-a^{(L)})^2 $

    - $\sigma(x)=\frac{1}{1-e^{-x}} , 其中，\sigma^{'}(x)=(1-\sigma(x)) \cdot \sigma(x)$

    - 对于输出层（l=L）: 
    $$ \delta^l = \frac{\partial E}{\partial z^l}
    =\frac{\partial E}{\partial a^l}\cdot \frac{\partial a^l}{\partial z^l}
    =(y-a^l)\cdot a^l\cdot (1-a^l)$$

    - 对于隐藏层（l<L）：
    $$ \delta^l = \frac{\partial E}{\partial z^l_i}
    = \frac{\partial E}{\partial z^{l+1}_1} \cdot \frac{\partial z^{l+1}_1}{\partial z^{l}_i}+\frac{\partial E}{\partial z^{l+1}_2} \cdot \frac{\partial z^{l+1}_2}{\partial z^{l}_i} + ... 
    = \sum_{k=1}^{d(l+1)}\frac{\partial E}{\partial z_{k}^{l+1}} \cdot \frac{\partial z_{k}^{l+1}}{\partial z_i^l} 
    = \sum_{k=1}^{d(l+1)}\frac{\partial E}{\partial z_{k}^{l+1}} \cdot \frac{\partial z_{k}^{l+1}}{\partial a_i^l} \cdot \frac{\partial a_i^l}{\partial z_i^l} 
    = (\sum_{k=1}^{d(l+1)} \delta_k^{l+1} \cdot w_{ki}^{l+1}) \cdot \sigma^{'}(z_i^l), 其中，d表示该层神经元的个数$$

- 后向传播算法伪代码。
<img src="https://ws4.sinaimg.cn/large/0069RVTdly1fupi6yooa5j30xq0nagph.jpg" width="40%" height="40%" />

## 三、代码实现
介绍完理论，必须实现一波，才能让自己觉得自己已经掌握了[\smile cry]。见[^2]

**参考**
[Michael Nielsen的博客中的chap 2](http://neuralnetworksanddeeplearning.com/chap2.html)
[^2]: [Network Code](https://github.com/edvardHua/Articles)

    